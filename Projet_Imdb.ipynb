{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1CbY0LCpr3e",
        "outputId": "a5aead3f-dfea-4daa-c9e5-f52e24817b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (2.1.0+cu121)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->torchdiffeq) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->torchdiffeq) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torchdiffeq\n",
        "import torchdiffeq\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqMOkR-DwdsS",
        "outputId": "47282dc1-c762-48cc-f483-6865569d7304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construction du modèle"
      ],
      "metadata": {
        "id": "XgS4lSiZ5h51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models import ODEBlock\n",
        "from plots import single_feature_plt\n",
        "from dataloaders import ConcentricSphere\n",
        "from training import Trainer\n",
        "from plots import get_feature_history\n",
        "from torch.utils.data import DataLoader\n",
        "from plots import multi_feature_plt\n",
        "from plots import trajectory_plt\n"
      ],
      "metadata": {
        "id": "X6UUpeEqp8Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderMHAttentionODEFunc(nn.Module):\n",
        "    \"\"\"MLP modeling the derivative of FeedForward ODE system.\n",
        "    device : torch.device\n",
        "    data_dim : int\n",
        "        Dimension of data.\n",
        "    hidden_dim : int\n",
        "        Dimension of hidden layers.\n",
        "    augment_dim: int\n",
        "        Dimension of augmentation. If 0 does not augment ODE, otherwise augments\n",
        "        it with augment_dim dimensions.\n",
        "    \"\"\"\n",
        "    def __init__(self, device, data_dim,  augment_dim=0, n_heads=1):\n",
        "        super(EncoderMHAttentionODEFunc, self).__init__()\n",
        "        self.device = device\n",
        "        self.augment_dim = augment_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.input_dim = data_dim + augment_dim\n",
        "        self.nfe = 0  # Number of function evaluations\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.att = nn.MultiheadAttention(self.input_dim, self.n_heads)\n",
        "        self.laynorm = nn.LayerNorm(self.input_dim)\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        \"\"\"\n",
        "        t : torch.Tensor (not used here)\n",
        "            Current time. Shape (1,).\n",
        "        x : torch.Tensor\n",
        "            Shape (batch_size, input_dim)\n",
        "        \"\"\"\n",
        "        # Forward pass of model corresponds to one function evaluation, so\n",
        "        # increment counter\n",
        "        self.nfe += 1\n",
        "        x = self.att(x, x, x)[0]\n",
        "        x = self.laynorm(x)\n",
        "\n",
        "\n",
        "class FeedForwardODEFunc(nn.Module):\n",
        "    \"\"\"MLP modeling the derivative of FeedForward ODE system.\n",
        "    device : torch.device\n",
        "    data_dim : int\n",
        "        Dimension of data.\n",
        "    hidden_dim : int\n",
        "        Dimension of hidden layers.\n",
        "    augment_dim: int\n",
        "        Dimension of augmentation. If 0 does not augment ODE, otherwise augments\n",
        "        it with augment_dim dimensions.\n",
        "    \"\"\"\n",
        "    def __init__(self, device, data_dim, hidden_dim, augment_dim=0):\n",
        "        super(FeedForwardODEFunc, self).__init__()\n",
        "        self.device = device\n",
        "        self.augment_dim = augment_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.input_dim = data_dim + augment_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.nfe = 0  # Number of function evaluations\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, self.input_dim),\n",
        "            nn.LayerNorm(self.input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        \"\"\"\n",
        "        t : torch.Tensor (not used here)\n",
        "            Current time. Shape (1,).\n",
        "        x : torch.Tensor\n",
        "            Shape (batch_size, input_dim)\n",
        "        \"\"\"\n",
        "        # Forward pass of model corresponds to one function evaluation, so\n",
        "        # increment counter\n",
        "        self.nfe += 1\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "9gAp4vmcqUzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ODETransformerClassification(nn.Module):\n",
        "  def __init__(self, device, seq_len, emb_size,  hidden_dim, n_blocks = 1,  augment_dim=0, n_heads = 1):\n",
        "    super(ODETransformerClassification, self).__init__()\n",
        "    self.device = device\n",
        "    self.seq_len = seq_len\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.augment_dim = augment_dim\n",
        "    self.n_heads = n_heads\n",
        "    ode_blocks = []\n",
        "    for i in range(n_blocks):\n",
        "      encoder_func = EncoderMHAttentionODEFunc(device, emb_size+ i*augment_dim , augment_dim, n_heads)\n",
        "      ode_blocks.append(ODEBlock(device, encoder_func, is_conv=True))\n",
        "      ff_func = FeedForwardODEFunc(device, emb_size+ i*augment_dim, hidden_dim, augment_dim=augment_dim)\n",
        "      ode_blocks.append(ODEBlock(device, ff_func, is_conv=True))\n",
        "\n",
        "    self.block_layers = nn.Sequential(*ode_blocks, nn.Flatten(), nn.Linear((emb_size+augment_dim*n_blocks)*seq_len, 1), nn.Sigmoid())\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.block_layers(x)\n"
      ],
      "metadata": {
        "id": "4SGnQxQBrzV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importation des données"
      ],
      "metadata": {
        "id": "1BZT96YG5mgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext as tt\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove = tt.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer = tt.data.utils.get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Define fields for text and label\n",
        "TEXT = tt.data.Field(lower=True, include_lengths=True, batch_first=True, tokenize=tokenizer)\n",
        "LABEL = tt.data.Field(sequential=False)\n",
        "\n",
        "# Define batch size, maximum review length, and maximum vocabulary words\n",
        "batch_size = 64\n",
        "max_review_len = 100\n",
        "max_vocab_words = 3500\n",
        "\n",
        "# Load IMDb dataset\n",
        "train_ds, test_ds = tt.datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_ds, max_size=max_vocab_words-2)\n",
        "LABEL.build_vocab(train_ds)\n",
        "\n",
        "# Split train dataset into train and dev sets\n",
        "train_ds, dev_ds = train_ds.split(split_ratio=0.8)\n",
        "\n",
        "# Create data iterators\n",
        "train_loader, dev_loader, test_loader = tt.data.BucketIterator.splits(\n",
        "    (train_ds, dev_ds, test_ds),\n",
        "    batch_sizes=(batch_size, batch_size, batch_size),\n",
        "    shuffle=True,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        ")"
      ],
      "metadata": {
        "id": "cmRQz0DjCB6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 150\n",
        "# Convert tokenized text into embedded tensors\n",
        "def get_embedded_text(loader):\n",
        "    embedded_texts = []\n",
        "    targets = []\n",
        "    for batch in loader:\n",
        "        text, lengths = batch.text\n",
        "        target = batch.label.unsqueeze(1)\n",
        "        target = torch.where(target == 1, torch.tensor(0.), torch.tensor(1.))  # Add dimension for concatenation\n",
        "\n",
        "\n",
        "        embedded_text = []\n",
        "        for sentence, length in zip(text, lengths):\n",
        "            sentence_emb = []\n",
        "            for i in range(min(length.item(), max_length)):\n",
        "                word = TEXT.vocab.itos[sentence[i]]\n",
        "                try:\n",
        "                    word_emb = glove.vectors[glove.stoi[word]]\n",
        "                except KeyError:\n",
        "                    word_emb = glove.vectors[glove.stoi['unk']]\n",
        "                sentence_emb.append(word_emb)\n",
        "            # Padding\n",
        "            sentence_emb += [torch.zeros(glove.vectors.shape[1])] * (max_length - length)\n",
        "            embedded_text.append(torch.stack(sentence_emb))\n",
        "\n",
        "        embedded_texts.append(torch.stack(embedded_text))\n",
        "        targets.append(target)\n",
        "\n",
        "    embedded_texts = torch.cat(embedded_texts)\n",
        "    targets = torch.cat(targets)\n",
        "    return embedded_texts, targets\n",
        "\n",
        "\n",
        "# Get embedded tensors for train, dev, and test datasets\n",
        "train_embedded = get_embedded_text(train_loader)\n",
        "print(\"Train embedded text tensor shape:\", train_embedded[0].shape)\n",
        "\n",
        "dev_embedded = get_embedded_text(dev_loader)\n",
        "print(\"Dev embedded text tensor shape:\", dev_embedded[0].shape)\n",
        "\n",
        "test_embedded = get_embedded_text(test_loader)\n",
        "print(\"Test embedded text tensor shape:\", test_embedded[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qGxeRFqCQJq",
        "outputId": "86ed15ef-7704-4d6f-9e3e-44dc8b3a7adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train embedded text tensor shape: torch.Size([20000, 150, 100])\n",
            "Dev embedded text tensor shape: torch.Size([5000, 150, 100])\n",
            "Test embedded text tensor shape: torch.Size([25000, 150, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.text = data[0]\n",
        "        self.target = data[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.text.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.text[idx], self.target[idx]\n",
        "\n",
        "\n",
        "trainset_embedded = CustomDataset(train_embedded)\n",
        "devset_embedded = CustomDataset(dev_embedded)\n",
        "testset_embedded = CustomDataset(test_embedded)\n",
        "\n",
        "traindataloader_embedded = DataLoader(trainset_embedded, batch_size=32, shuffle=True)\n",
        "testdataloader_embedded = DataLoader(testset_embedded, batch_size=64, shuffle=True)\n",
        "devdataloader_embedded = DataLoader(devset_embedded, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "Xy-ywYStGIQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrainement et test"
      ],
      "metadata": {
        "id": "aNphhF0H5pLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "model = ODETransformerClassification(device, max_length, 100, 32)\n",
        "\n",
        "def train(dataloader, model, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        inputs, targets = batch\n",
        "        inputs = inputs.to(device)  # Move inputs to GPU if available\n",
        "        targets = targets.to(device)  # Move targets to GPU if available\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs.view(-1), targets.view(-1))  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        total_loss += loss.item() * len(inputs)\n",
        "        total_samples += len(inputs)\n",
        "\n",
        "    return total_loss / total_samples\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(20):\n",
        "  loss = train(traindataloader_embedded, model, optimizer, criterion)\n",
        "  print(f\"Epoch {i+1} : Loss = {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX4GHB6_5gIA",
        "outputId": "aac8bb24-2465-4ea2-fe54-bed968958495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [23:14<00:00,  2.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 : Loss = 55.24475383300781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [27:59<00:00,  2.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 : Loss = 55.25598328857422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 405/625 [20:37<11:08,  3.04s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total_loss = 0\n",
        "  total = 0\n",
        "  for inputs, targets in tqdm(devdataloader_embedded):\n",
        "    outputs = model(inputs.to(device))\n",
        "    total_loss += criterion(outputs.view(-1), targets.view(-1))*len(inputs)\n",
        "    preds = torch.where(outputs.view(-1) <0.5, torch.tensor(1.), torch.tensor(1.))\n",
        "    correct += torch.sum(preds==targets.view(-1)).item()\n",
        "    total += len(inputs)\n",
        "\n",
        "print(f\"Dev loss : {total_loss/total}\")\n",
        "print(f\"Dev accuracy : {correct/total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh8qEtXkaaps",
        "outputId": "890bf6f8-8df5-4ba0-b531-c6f63e4c03d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [02:33<00:00,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev loss : 128.8616485595703\n",
            "Dev accuracy : 0.507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}